<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Multi-View Domain Adaptation for Nighttime Aerial Tracking">
  <meta name="keywords" content="UAV, Tracking, Domain Adaptation, Nighttime, Multi-View">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multi-View Domain Adaptation for Nighttime Aerial Tracking</title>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/misc.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-fullhd">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Multi-View Domain Adaptation for Nighttime Aerial Tracking</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#">Haoyang Li</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="#">Guangze Zheng</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="#">Sihang Li</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="#">Junjie Ye</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="#">Changhong Fu</a><sup>1,*</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Tongji University,</span>
              <span class="author-block"><sup>2</sup>The University of Hong Kong</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/your_paper.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/vision4robotics/MVDANT" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://your_dataset_link" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          <span class="dnerf">MVDANT</span> performs multi-view domain adaptation for nighttime aerial tracking with high precision and robustness.
        </h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/your_video_id" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </section>

  <section class="hero">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <div class="container is-max-widescreen">

          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We present a multi-view domain adaptation framework for nighttime aerial tracking named MVDANT. Our approach addresses the challenges of adapting daytime tracking models to nighttime scenarios while considering multiple viewpoints. MVDANT combines multi-view knowledge fusion, feature alignment, and adversarial learning to bridge the gap between daytime and nighttime domains.
            </p>
            <p>
              The framework includes a novel multi-view feature aligner with a transformer structure and a Transformer-based hierarchical discriminator. These components work together to capture diverse perspectives and lighting distribution knowledge, improving the robustness of tracking objects from various views.
            </p>
            <p>
              Our experimental results demonstrate superior performance on challenging nighttime UAV benchmarks, with significant improvements in precision, normalized precision, and success rate compared to state-of-the-art trackers.
            </p>
          </div>

          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">

              <h2 class="title is-3">Prototype</h2>

              <div class="columns is-centered has-text-centered">
                <div class="column is-fullwidth">
                  <img src="./images/Product.png" class="interpolation-image" alt="" />
                  <p style="text-align:center">Prototype Product</p>
                </div>
                <div class="column is-fullwidth">
                  <video id="teaser" autoplay muted loop playsinline height="100%">
                    <source src="./images/demo.mp4" type="video/mp4">
                  </video>
                  <p style="text-align:center">Clinical Demonstration</p>
                </div>
              </div>

              <div class="columns is-centered has-text-centered">
                <div class="column is-fullwidth">
                  <img src="./images/prototype_combine.png" alt="Vein Puncture" class="interpolation-image">
                  <p style="text-align:center">Vein Puncture Real Robot</p>
                </div>
              </div>

            </div>
          </div>

          <h2 class="title is-3">Robot Design</h2>
          <div class="content has-text-justified">
            <p>
              Our venipuncture robot features a 6-DOF compact mechanism designed for precise vascular access. The
              robot's structure is built around a reliable short transmission chain, primarily consisting of precision
              ball screws and Maxon motors, ensuring high mechanical efficiency and accuracy.
            </p>
            <p>
              The motor system comprises six key components:
            </p>
            <ul>
              <li>Motors 1, 2, and 3 operate jointly for x-y plane movement</li>
              <li>Motor 4 determines the puncture angle</li>
              <li>Motor 5 controls needle movement along the y-direction</li>
              <li>Motor 6 advances the needle to the vein puncture point</li>
            </ul>
          </div>

          <div class="column is-fullwidth">
            <img src="./images/design_cad.png" class="interpolation-image" alt="Motor System Design" />
            <p style="text-align:center">Motor System Design</p>
          </div>

          <div class="content has-text-justified">
            <p>
              A key feature is the 3-DOF Needle Module, which includes:
            </p>
            <ul>
              <li>A linear stage driven by a lead screw spindle</li>
              <li>A worm drive system for independent insertion angle control</li>
              <li>A self-locking rotational joint for safety</li>
              <li>Two ball screw mechanisms for precise needle movement</li>
            </ul>
          </div>

          <div class="column is-fullwidth">
            <img src="./images/puncture_combine.png" class="interpolation-image" alt="" />
            <p style="text-align:center">Puncture Unit</p>
          </div>

          <div class="content has-text-justified">
            <p>
              This design allows for accurate adjustment of puncture speed and dramatically improves puncture
              stabilityand needle tip positioning accuracy.
            </p>
          </div>

          <div class="columns is-centered has-text-centered">
            <div class="column is-fullwidth">
              <img src="./images/product_design_1.png" alt="Product Design 1" class="interpolation-image">
              <p style="text-align:center">Product Design 1</p>
            </div>
            <div class="column is-fullwidth">
              <img src="./images/product_design_2.png" alt="Product Design 2" class="interpolation-image">
              <p style="text-align:center">Product Design 2</p>
            </div>
          </div>

          <h2 class="title is-3">Vein Detection</h2>
          <div class="content has-text-justified">
            <h3 class="title is-4">Near Infrared Imaging</h3>
            <p>
              The robot employs Near Infrared (NIR) imaging technology for accurate vein detection. This non-invasive
              method allows for clear visualization of superficial veins, enhancing the robot's ability to identify
              suitable puncture sites.
            </p>
          </div>

          <div class="column is-fullwidth">
            <img src="./images/NIR.png" alt="NIR Method - Mixer U-Net" class="interpolation-image">
            <p style="text-align:center">NIR Segmentation</p>
          </div>

          <div class="content has-text-justified">
            <p>
              We randomly select six NIR images as examples and show the vein segmentation and suitable puncture areas
              in Fig. 8. We propose a Dual-In-Dual-Out network with two-step learning and two-task learning to
              determine the suitable puncture area and angle from the NIR image inputs. A visual illustration of the
              proposed
              network is shown in Fig. 9. It contains two steps of training: first, it trains a Single-In-Single-Out
              network to segment the vein from the NIR image; second, it inputs both the NIR image and vein
              segmentation from the first step training into the Dual-In-Dual-Out network to regress the suitable
              puncture area and
              angle.
            </p>
          </div>

          <div class="columns is-centered has-text-centered">
            <div class="column is-fullwidth">
              <img src="./images/near_infrared_results.png" alt="NIR Results" class="interpolation-image">
              <p style="text-align:center">NIR Results</p>
            </div>
            <div class="column is-fullwidth">
              <img src="./images/infra.gif" alt="NIR Animation" class="interpolation-image">
              <p style="text-align:center">NIR Animation</p>
            </div>
          </div>

          <div class="content has-text-justified">
            <p>
              Five examples of the suitable puncture area regression by the four methods are shown in Fig. 10. We can
              visually see that both the Dual-In-Single-Out and Dual-In-Dual-Out network can distinguish between the
              suitable and non-suitable puncture area better than the Single-In-Single-Out and Single-In-Dual-Out
              network, indicating the importance and value of bringing the vein segmentation into the network's input.
              For the regression of suitable puncture areas and angle, the mean and std DSC are shown in Table Ⅰ and
              Table Ⅱ.
            </p>
          </div>

          <div class="content has-text-justified">
            <h3 class="title is-4">Near Infrared Imaging</h3>
            <p>
              Complementing the NIR system, ultrasound technology is used for precise depth estimation and
              longitudinal
              vein imaging. The ultrasound device (st-1c transducer, frequency 7.5MHz) provides high-resolution images
              of vein cross-sections, enabling accurate calculation of puncture depth.
            </p>
          </div>

          <div class="column is-fullwidth">
            <img src="./images/probe.png" alt="Ultrasound Method - ISM" class="interpolation-image">
            <p style="text-align:center">Ultrasound System</p>
          </div>

          <div class="content has-text-justified">
            <p>
              An illustration of the dataset is shown in Fig. 11, where green boxes illustrate the veins, yellow boxes
              illustrate the vein shadow and red boxes represent the vessel edges. In previous studies, UNet, FPN, and
              other models have shown good performance in image segmentation. To merge the advantages of these
              deep-learning neural networks, our model integrates multiple image segmentation networks. Through
              stacking
              methods and feature image coding, we propose the Integrated Segmentation Model (ISM), offering high
              precision for vein segmentation from ultrasonic images. As shown in Fig. 12, the overall structure of
              ISM
              includes two layers. The first layer is composed of three sub-models (FPN, PSPNet, and UNet, numbered as
              models 1, 2, and 3). The output result diagram of the first layer was used as the input training picture
              of the second layer. The second layer of the sub-model is composed of LinkNet, which takes the label
              picture (GT) of the original data set as the recognition target.
            </p>
          </div>

          <div class="columns is-centered has-text-centered">
            <div class="column is-fullwidth">
              <img src="./images/heatmap.png" alt="Ultrasound Results" class="interpolation-image">
            </div>
            <!-- <div class="column is-fullwidth">
              <img src="./images/ultra.gif" alt="Ultrasound Animation" class="interpolation-image">
            </div> -->
          </div>

          <div class="content has-text-justified">
            <p>
              As can be seen in Table II, the proposed ISM demonstrates its superiority over traditional models in
              terms
              of the three indicators, namely Dice-Similarity-Coefficient (DSC), Hausdorff-Distance (HD95) and
              Intersection-Over-Union (IOU). The ISM model achieved significant improvements in multiple indicators:
              its DSC value increased by about 6 %, reaching 94.62 %, and the IOU value increased by about 11%, in
              complex
              samples. Finally, a clear puncture point is calculated through the connection domain algorithm.
              Experimentally, the success rate of selecting the suitable vein for puncture is 99.21%.
            </p>
          </div>

          <div class="content has-text-justified">
            <p>
              Training procedure of the FPN, UNet, PSPNet and ISM: (a) Dice Loss curve of FPN, UNet, PSPNet, ISM. (b)
              ROC-AUC curve of FPN, UNet, PSPNet, ISM of testing picture. (c) The DSC curve of vein segmentation
              results using different models: FPN, UNet, PSPNet, LinkNet, ISM.
            </p>
          </div>

          <div class="column is-fullwidth">
            <img src="./images/loss.png" alt="Experimental Results" class="interpolation-image">
            <p style="text-align:center">Experimental Results</p>
          </div>

          <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified">
            <p>
              Our MVDANT framework consists of three main components:
            </p>
            <ul>
              <li>Feature Alignment: Using a multi-view feature aligner with a novel transformer structure</li>
              <li>Tracker Alignment: Employing discriminators for each perspective to distinguish daytime from nighttime images</li>
              <li>Overall Objective: Combining classification, regression, adversarial, and consistency losses</li>
            </ul>
          </div>

          <div class="column is-fullwidth">
            <img src="./images/pipeline.png" class="interpolation-image" alt="MVDANT Overview" />
            <p style="text-align:center">Overview of MVDANT</p>
          </div>

          <h2 class="title is-3">Results</h2>
          <div class="content has-text-justified">
            <p>
              MVDANT outperforms state-of-the-art trackers on challenging nighttime UAV benchmarks:
            </p>
            <ul>
              <li>2.6% improvement in success rate on NAT2021-test</li>
              <li>1.2% improvement in success rate on UAVDark70</li>
              <li>Significant improvements in long-term tracking performance on NAT2021-L-test</li>
            </ul>
          </div>

          <div class="column is-fullwidth">
            <img src="./images/rainbow.png" class="interpolation-image" alt="Performance Comparison" />
            <p style="text-align:center">Performance Comparison on Nighttime Aerial Tracking Benchmarks</p>
          </div>

          <h2 class="title is-3">Real-World Tests</h2>
          <div class="content has-text-justified">
            <p>
              MVDANT was implemented on a typical embedded system, the NVIDIA Jetson AGX Xavier, to demonstrate its applicability in nighttime drone tracking applications in the real world. Without TensorRT acceleration, MVDANT achieves an impressive real-time speed of 31.25 frames per second (FPS). The following videos showcase our real-world tests, demonstrating the robustness and effectiveness of MVDANT in various nighttime tracking scenarios.
            </p>
          </div>

          <div class="columns is-centered has-text-centered">
            <div class="column is-one-third">
              <video controls class="interpolation-image">
                <source src="./images/test1__82pct_smaller.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
              <p style="text-align:center">Real-World Test 1</p>
            </div>
            <div class="column is-one-third">
              <video controls class="interpolation-image">
                <source src="./images/test2__85pct_smaller.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
              <p style="text-align:center">Real-World Test 2</p>
            </div>
            <div class="column is-one-third">
              <video controls class="interpolation-image">
                <source src="./images/test3__87pct_smaller.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
              <p style="text-align:center">Real-World Test 3</p>
            </div>
          </div>

        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{li2023multiview,
  title={Multi-View Domain Adaptation for Nighttime Aerial Tracking},
  author={Li, Haoyang and Zheng, Guangze and Li, Sihang and Ye, Junjie and Fu, Changhong},
  journal={arXiv preprint arXiv:2023.xxxxx},
  year={2023}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <p>
          Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>.
        </p>
      </div>
    </div>
  </footer>

</body>

</html>