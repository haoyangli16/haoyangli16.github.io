<!DOCTYPE html>
<html>

<head>
    <title>Advanced Grasp Generation Techniques for Robotic Manipulation</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="Advanced Grasp Generation Techniques for Robotic Manipulation">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="hardware_research/sim2real/summary.html">
</head>

<body>

    <!-- Wrapper -->
    <div id="wrapper">
        <header id="header">
            <div class="inner">

                <!-- Logo -->
                <a href="/" class="logo">
                    <span class="symbol"><img src="/images/logo.png" alt="" /></span><span class="title">Robotic
                        Manipulation</span>
                </a>
                <!-- Nav -->
                <nav>
                    <ul>
                        <li><a href="#menu">Menu</a></li>
                    </ul>
                </nav>

            </div>
        </header>

        <nav id="menu">
            <h2>Menu</h2>
            <ul>
                <li><a href="/Publications.html">Publications</a></li>
                <li><a href="/Resume.html">CV</a></li>
                <li><a href="/"></a></li>
            </ul>
        </nav>

        <!-- Main -->
        <div id="main">
            <div class="inner">
                <h1>Sim-To-Real: From Imitation to Reinforcement and Beyond</h1>

                <h2>Abstract</h2>
                <p>This report details a comprehensive study on improving robot grasping capabilities, progressing from
                    simple imitation learning to sophisticated reinforcement learning techniques and visual-based
                    policies. Our experiments span various learning paradigms, input modalities, and model
                    architectures, offering insights into the strengths and limitations of each approach.</p>

                <h2>1. Introduction</h2>
                <p>Robot grasping remains a fundamental challenge in robotics, requiring precise perception, planning,
                    and control. This study aims to systematically explore and compare different learning approaches to
                    solve this task, with a focus on sample efficiency, generalization, and real-world applicability.
                </p>
                <video width="100%" max-width="800px" controls>
                    <source src="pick_cube_sim.mp4" type="video/mp4">
                </video>

                <h2>2. Methodology</h2>
                <p>Our research progressed through several key stages, each building upon the insights from the
                    previous:</p>

                <h3>2.1 Behavior Cloning with Diffusion Policies</h3>
                <p>We began with a pure imitation learning approach, leveraging recent advances in diffusion models for
                    behavior prediction.</p>

                <h4> (1) Experimental Setup:</h4>
                <ul>
                    <li><strong>Input Modalities:</strong> RGB images, later expanded to RGB-D</li>
                    <li><strong>Model Architecture:</strong> Transformer-based diffusion policy</li>
                    <li><strong>Dataset:</strong> 100 demonstrations (both simulated and real-world)</li>
                    <li><strong>Visual Encoders:</strong> ResNet18, CLIP (both global and local features)</li>
                </ul>

                <h4> (2) Key Explorations:</h4>
                <ul>
                    <li>Comparison of different visual encoders</li>
                    <li>Impact of depth information on grasping performance</li>
                    <li>Various action representations:
                        <ul>
                            <li>Sample position</li>
                            <li>Epsilon position</li>
                            <li>Sample delta</li>
                            <li>Epsilon delta</li>
                        </ul>
                    </li>
                </ul>

                <h4> (3) Results:</h4>
                <ul>
                    <li>Achieved moderate success in simulated environments</li>
                    <li>Real-world performance was limited, highlighting the reality gap</li>
                </ul>
                <div class="box alt">
                    <div class="row uniform">
                        <div class="6u"><span class="image fit"><video width="95%" max-width="600px" controls>
                                    <source src="test_17000.mp4" type="video/mp4">
                                </video></span>
                        </div>
                        <div class="6u$"><span class="image fit"><img src="success_dist.png" width="40%"
                                    max-width="400px" alt="" /></span>
                        </div>
                    </div>
                </div>

                <h3>2.2 PPO with Demonstration Augmented Policy Gradient (DAPG)</h3>
                <p>Building on the limitations of pure imitation, we explored combining reinforcement learning with
                    demonstrations.</p>

                <h4> (1) Experimental Setup:</h4>
                <ul>
                    <li><strong>Input:</strong> State-based (robot joint positions, object poses)</li>
                    <li><strong>Model:</strong> Multi-layer perceptron (MLP) policy network</li>
                    <li><strong>Algorithm:</strong> PPO + DAPG</li>
                </ul>

                <h4>(2) Key Explorations:</h4>
                <ul>
                    <li>Reward structures:
                        <ul>
                            <li>Dense reward (shaped for task completion)</li>
                            <li>Sparse reward (binary success signal)</li>
                        </ul>
                    </li>
                    <li>BC weight (balancing RL and imitation objectives)</li>
                    <li>Demonstration sources:
                        <ul>
                            <li>10 human demonstrations</li>
                            <li>100 human demonstrations</li>
                            <li>600 RL-generated demonstrations</li>
                            <li>3000 RL-generated demonstrations</li>
                        </ul>
                    </li>
                </ul>

                <h4>(3) Results:</h4>
                <ul>
                    <li>Dense reward led to successful grasping, but pure RL also succeeded</li>
                    <li>Sparse reward resulted in very slow learning</li>
                    <li>RL component seemed to dominate learning, questioning the value of demonstrations in this setup
                    </li>
                </ul>


                <h3>2.3 Residual Policy Learning</h3>
                <p>To leverage the strengths of both BC and RL, we developed a residual policy approach.</p>
                <div style="display: flex; justify-content: center;">
                    <img src="residual_policy.png" alt="Overview of robot grasping research"
                        style="width: 100%; max-width: 800px;">
                </div>

                <h4>(1) Experimental Setup:</h4>
                <ul>
                    <li><strong>Base Policy:</strong> Fixed BC state-based diffusion policy</li>
                    <li><strong>Residual Policy:</strong> State-based PPO + BC, trained with sparse reward</li>
                    <li><strong>Combined Action:</strong> a_total = a_base + w * a_residual</li>
                </ul>

                <h4>(2) Key Explorations:</h4>
                <ul>
                    <li>Residual weights (w): {0.004, 0.02, 0.1, 0.3}</li>
                    <li>Training epochs: 400 to 2000</li>
                    <li>Impact of residual normalization</li>
                </ul>

                <h4>(3) Results:</h4>
                <ul>
                    <li>Achieved >95% success rate in simulation</li>
                    <li>Significantly faster training compared to PPO+DAPG</li>
                    <li>Optimal residual weight found to be task-dependent</li>
                </ul>
                <div class="box alt">
                    <div class="row uniform">
                        <div class="6u"><span class="image fit"><img src="image_cleanup.png" alt="" /></span>
                        </div>
                        <div class="6u$"><span class="image fit"><img src="sim_merge_image.png" alt="" /></span>
                        </div>
                    </div>
                </div>

                <h3>2.4 State-to-Visual Policy Transfer (DAgger)</h3>
                <p>To bridge the gap between state-based and vision-based policies, we employed Dataset Aggregation
                    (DAgger).</p>

                <h4>(1) Experimental Setup:</h4>
                <ul>
                    <li><strong>Teacher:</strong> State-based policy (trained with residual PPO + BC)</li>
                    <li><strong>Student:</strong> Visual-based policy (CLIP local features, diffusion model)</li>
                    <li><strong>Alternative Student:</strong> Point cloud-based policy (PointNet++ encoder)</li>
                </ul>

                <h4>(2)Key Explorations:</h4>
                <ul>
                    <li>Iterative data collection and policy improvement</li>
                    <li>Comparison of image-based vs. point cloud-based representations</li>
                </ul>

                <h4>(3) Results:</h4>
                <ul>
                    <li>Training speed was slower than anticipated</li>
                    <li>Highlighted challenges in transferring state-based knowledge to visual domain</li>
                </ul>
                <video width="100%" max-width="600px" controls>
                    <source src="pick_mug.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>

                <h3>2.5 End-to-End Visual Grasping Policies</h3>
                <p>Finally, we focused on developing end-to-end visual grasping policies, aiming for real-world
                    applicability.</p>

                <h4> (1) Experimental Setup:</h4>
                <ul>
                    <li><strong>Input:</strong> RGB images (third-person view)</li>
                    <li><strong>Model:</strong> Transformer-based diffusion policy</li>
                    <li><strong>Visual Encoder:</strong> CLIP (local features)</li>
                </ul>

                <h4> (2) Key Explorations:</h4>
                <ul>
                    <li>Positional encodings: learned embeddings vs. sinusoidal</li>
                    <li>Hyperparameter tuning: learning rates, weight decay, batch sizes</li>
                    <li>Action sampling strategies</li>
                </ul>

                <h4> (3) Results:</h4>
                <ul>
                    <li>Achieved promising results in both simulation and real-world settings</li>
                    <li>Local CLIP features outperformed global features</li>
                    <li>Careful tuning of hyperparameters proved crucial for success</li>
                </ul>
                <video width="50%" max-width="300px" controls>
                    <source src="sim2real_demo_june_compress.mp4" type="video/mp4">
                </video>

                <h2>3. Discussion</h2>
                <p>Our systematic exploration revealed several key insights:</p>
                <ol>
                    <li>Pure imitation learning, while simple to implement, struggles with generalization to new
                        scenarios.</li>
                    <li>Combining RL with demonstrations can be powerful, but careful balancing is required.</li>
                    <li>Residual policy learning offers a promising direction, leveraging the strengths of both
                        imitation and reinforcement learning.</li>
                    <li>Bridging the gap between state-based and vision-based policies remains challenging, highlighting
                        the need for better transfer learning techniques.</li>
                    <li>End-to-end visual policies show promise for real-world applications, but require careful
                        architecture design and hyperparameter tuning.</li>
                </ol>

                <h2>4. Future Work</h2>
                <p>Based on our findings, several promising directions for future research emerge:</p>
                <ol>
                    <li>Exploring more sophisticated visual representations, potentially leveraging large
                        language-vision models.</li>
                    <li>Investigating multi-task and meta-learning approaches to improve generalization.</li>
                    <li>Developing more efficient exploration strategies for sparse reward settings.</li>
                    <li>Integrating tactile feedback to improve grasping precision and robustness.</li>
                </ol>

                <h2>5. Conclusion</h2>
                <p>This study provides a comprehensive overview of various learning approaches for robot grasping, from
                    imitation to reinforcement and beyond. By systematically exploring different techniques, we've
                    identified promising directions and key challenges in developing robust, generalizable grasping
                    policies. Our findings contribute to the ongoing effort to bridge the gap between simulated and
                    real-world robotic manipulation.</p>
            </div>

            <!-- add the divider -->
            <hr>

            <h1>Bridging Gaps in Robotic Grasping: Reflections on Our Journey</h1>

            <p>As I sit here, surrounded by robots and lines of code, I can't help but reflect on our recent adventures
                in the world of robotic grasping. Our journey has been nothing short of fascinating, filled with moments
                of frustration, surprise, and pure joy. But more than anything, it's been a journey of bridging gaps –
                three crucial transitions that I believe will shape the future of robotics.</p>

            <h2>1. From Simulation to Reality: The Unforgiving Leap</h2>

            <p>Our experiments began in the comfort of simulation. Clean, predictable, and forgiving. We started with
                behavior cloning, teaching our robots to mimic human demonstrations. It was like teaching a child to
                draw by guiding their hand – effective, but limited.</p>

            <p>As we moved to reinforcement learning techniques like PPO+DAPG, our simulated robots became more adept,
                learning to grasp objects with increasing precision. But then came the moment of truth – the leap to the
                real world.</p>

            <p>The transition was humbling. Shadows played tricks on our vision systems. Imperfect motors didn't move
                exactly as commanded. The real world, in all its messy glory, exposed the limitations of our simulated
                training.</p>

            <p>This gap between simulation and reality is more than just a technical challenge. It's a reminder of the
                complexity of the world we're trying to navigate. As we push forward, I'm excited about the potential of
                domain randomization and sim-to-real transfer techniques. Perhaps the key lies not in perfect
                simulation, but in embracing and learning from the imperfections?</p>

            <h2>2. From State to Vision: Seeing the World Anew</h2>

            <p>Another significant transition in our research was moving from state-based inputs to visual perception.
                Initially, our robots knew the exact position of every joint, every object. It was like playing chess
                with perfect information.</p>

            <p>The shift to visual inputs – first RGB, then RGB-D – was like asking our robots to play chess by looking
                at the board, just as humans do. Our experiments with various visual encoders, from ResNet to CLIP, felt
                like giving our robots new eyes, each with its own way of seeing the world.</p>

            <p>This transition forces us to grapple with fundamental questions about perception and action. How do we
                distill the complexity of visual information into meaningful actions? Our work with end-to-end visual
                policies is just the beginning. I'm particularly excited about the potential of large visual-language
                models in this space. Could we one day have robots that not only see but truly understand their
                environment?</p>

            <h2>3. From Imitation to Reinforcement: Learning to Explore</h2>

            <p>Perhaps the most profound transition was from pure imitation learning to reinforcement learning, and
                eventually to hybrid approaches like our residual policy method.</p>

            <p>We started with behavior cloning – safe, predictable, but limited by the quality of demonstrations.
                Moving to reinforcement learning was like watching our robots grow up, learning to explore and make
                mistakes. The sparse reward experiments were particularly telling – like watching a toddler learn to
                walk, with moments of frustration but also breakthroughs.</p>

            <p>Our residual policy approach, combining a fixed behavior-cloned policy with a learned RL component, feels
                like a metaphor for human learning. We start by imitating, then gradually learn to improvise and adapt.
            </p>

            <p>This journey from imitation to reinforcement learning reflects a deeper question in AI and robotics: How
                do we balance the knowledge we can impart with the need for autonomous learning? As we push towards more
                general-purpose robots, this balance will be crucial.</p>

            <h2>Looking Ahead: The Gaps Yet to Bridge</h2>

            <p>As I ponder these transitions – sim-to-real, state-to-visual, imitation-to-reinforcement – I'm struck by
                how they mirror broader challenges in AI and robotics. Each represents a form of generalization, a step
                towards more adaptable, robust systems.</p>

            <p>Looking ahead, I'm excited about several possibilities:</p>

            <ol>
                <li><strong>Multimodal Learning</strong>: Combining vision, touch, and even sound for more robust
                    grasping.</li>
                <li><strong>Continual Learning</strong>: Developing robots that don't just learn a task but continue to
                    improve over time.</li>
                <li><strong>Semantic Understanding</strong>: Moving beyond pure perception to true comprehension of
                    objects and their uses.</li>
            </ol>

            <p>Our journey in robotic grasping has been more than a series of experiments. It's been a window into the
                fundamental challenges of creating machines that can interact with our world as fluently as we do. As we
                continue to bridge these gaps, I'm reminded that each challenge overcome brings us one step closer to
                robots that can truly assist and augment human capabilities in meaningful ways.</p>

            <p>The gaps we're bridging in robotic grasping are, in many ways, the gaps between artificial and human
                intelligence. It's a journey that's far from over, but one that promises to revolutionize not just
                robotics, but our understanding of intelligence itself.</p>
        </div>

        <!-- Footer -->
        <footer id="footer">
            <div class="inner">
                <section>
                    <h2>Follow</h2>
                    <ul class="icons">
                        <li><a href="https://github.com/haoyangli16" class="icon style2 fa-github"><span
                                    class="label">GitHub</span></a></li>
                        <li><a href="mailto:hal168@ucsd.edu" class="icon style2 fa-envelope-o"><span
                                    class="label">Email</span></a></li>
                    </ul>
                </section>
                <ul class="copyright">
                    <li>&copy; Robotic Manipulation Research. All rights reserved</li>
                    <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
                    <li>Jekyll integration: <a href="http://andrewbanchi.ch">Andrew Banchich</a></li>
                </ul>
            </div>
        </footer>
    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/skel.min.js"></script>
    <script src="assets/js/util.js"></script>
    <!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
    <script src="assets/js/main.js"></script>

</body>

</html>